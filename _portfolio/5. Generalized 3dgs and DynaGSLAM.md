---
title: "Dynamic 3D Gaussian Splatting SLAM for Robot Vision"
excerpt: "Integrating Generalized 3DGS with DynaGSLAM for robust dynamic scene reconstruction<br/><img src='/images/dynamic-3dgs-thumbnail.jpg'>"
collection: portfolio
---

## Overview

Course project for Robot Vision (Fall 2025) at Seoul National University, focused on advancing real-time 3D scene reconstruction in dynamic environments. This research integrates the generalization capabilities of Generalized 3D Gaussian Splatting with the dynamic scene handling of DynaGSLAM to create a robust SLAM system suitable for robotic applications.

<img src="/images/dynamic-3dgs-main.jpg" alt="Dynamic 3DGS SLAM System" style="width:100%; max-width:800px; margin: 20px 0;">

## Research Motivation

Modern robotic systems require real-time perception in dynamic environments, but existing SLAM methods face critical limitations:

**Current Challenges:**
- **Static scene assumption**: Traditional 3DGS methods assume static scenes, failing in dynamic environments
- **Generalization**: Poor performance on unseen scene types without extensive retraining
- **Computational efficiency**: Real-time constraints for robotic deployment
- **Dynamic object handling**: Moving objects cause tracking failures and map corruption

**Application Domains:**
- Autonomous navigation in crowded environments
- Human-robot interaction scenarios
- Dynamic warehouse and manufacturing settings
- Mobile robotics in everyday spaces

## Technical Approach

### 1. Generalized 3D Gaussian Splatting Foundation

Building on the recent Generalized 3DGS framework (2024) which enables:
- Cross-scene generalization without per-scene optimization
- Efficient scene representation using 3D Gaussians
- Real-time rendering capabilities
- Learned scene priors from diverse training data

**Key Advantages:**
- Reduced per-scene adaptation time
- Better handling of novel scene geometries
- Improved initialization for SLAM pipeline

### 2. DynaGSLAM Integration

Incorporating dynamic scene handling capabilities from DynaGSLAM:
- Explicit dynamic object segmentation and tracking
- Temporal consistency in 3D Gaussian representations
- Motion-aware camera pose estimation
- Robust loop closure in dynamic scenarios

**Technical Components:**
- Motion segmentation network for dynamic object detection
- Separate tracking for static background and moving objects
- Temporal regularization of Gaussian parameters
- Adaptive keyframe selection strategy

### 3. Hybrid Architecture Design

**System Pipeline:**

```
RGB-D Input → Feature Extraction → Dynamic Segmentation
                                          ↓
                              Static GS ← → Dynamic GS
                                    ↓
                            Pose Estimation ← Generalized Priors
                                    ↓
                            Map Optimization
```

**Novel Contributions:**
- Leveraging generalized priors for faster convergence in new scenes
- Joint optimization of static and dynamic scene components
- Efficient Gaussian pruning strategy for real-time performance
- Uncertainty-aware Gaussian initialization using learned features

### 4. Implementation Details

**Network Architecture:**
- Encoder: ResNet-based feature extractor pre-trained on diverse scenes
- Generalization module: Transformer-based scene prior network
- Dynamic segmentation: Lightweight U-Net variant for real-time inference
- Gaussian decoder: MLP networks for parameter prediction

**Optimization Strategy:**
- Two-stage training: (1) Generalized prior learning, (2) SLAM fine-tuning
- Adaptive learning rates for static vs. dynamic components
- Photometric and geometric loss with temporal consistency terms

## Expected Outcomes

**Technical Goals:**
- Reduce scene adaptation time by 50% compared to vanilla 3DGS SLAM
- Maintain < 30ms per-frame latency for real-time operation
- Achieve robust tracking in scenes with 30%+ dynamic content
- Demonstrate generalization across indoor/outdoor environments

**Evaluation Metrics:**
- Camera tracking accuracy (ATE, RPE)
- Map reconstruction quality (F-score, precision/recall)
- Generalization performance on unseen scene types
- Computational efficiency (FPS, memory footprint)

## Project Timeline

**September 2025**: Literature review, dataset preparation, baseline implementation
**October 2025**: Generalized 3DGS integration, initial training pipeline
**November 2025**: DynaGSLAM fusion, dynamic scene handling
**December 2025**: Extensive evaluation, ablation studies, final report

<img src="/images/dynamic-3dgs-timeline.png" alt="Project Timeline" style="width:100%; max-width:700px; margin: 20px 0;">

## Datasets & Benchmarks

**Training Data:**
- ScanNet: Indoor scenes with diverse geometries
- Replica: High-quality synthetic environments
- TUM RGB-D: Dynamic sequence benchmark

**Evaluation Benchmarks:**
- TUM RGB-D dynamic sequences
- Bonn RGB-D Dynamic dataset
- Custom lab recordings with known ground truth

## Technologies & Tools

**Deep Learning**: PyTorch, PyTorch3D, CUDA  
**3D Vision**: Open3D, OpenCV, COLMAP  
**Gaussian Splatting**: Official 3DGS codebase, gsplat library  
**SLAM**: ORB-SLAM3 baseline comparison  
**Visualization**: Wandb, Matplotlib, Rerun viewer

## Expected Impact

This project aims to demonstrate that combining learned scene priors with dynamic scene understanding can significantly advance the state-of-the-art in robotic vision. The resulting system could enable more robust autonomous navigation and human-robot collaboration in real-world environments.

## Project Resources

[GitHub Repository (TBD)](https://github.com/toddjrdl/dynamic-3dgs-slam) | [Project Proposal](/files/dynamic-3dgs-proposal.pdf) | [Weekly Progress Updates](/blog/robot-vision-2025/)

---

**Duration**: September 2025 - December 2025  
**Course**: Robot Vision (Graduate Level), Seoul National University  
**Instructor**: [Professor Name]  
**Team Size**: Individual project  
**Status**: In Progress