---
title: "Humanoid Locomotion with RL: TOCABI Bipedal Robot"
excerpt: "Reinforcement learning pipeline for stable bipedal walking with privileged observations and sim2real transfer<br/><img src='/images/tocabi-thumbnail.jpg'>"
collection: portfolio
---

## Overview

Research internship at [DYROS](http://dyros.snu.ac.kr/) (Seoul National University) investigating reinforcement learning methods for bipedal humanoid locomotion across diverse terrains. This project explored multiple RL frameworks to enable humanoid(TOCABI) to navigate challenging environments using RGBD sensor-based terrain perception.

**Institution**: Dynamic Robotics Systems Lab, Seoul National University  
**Duration**: July 2025 - September 2025  
**Advisor**: Professor Jaeheung Park

<img src="/images/tocabi-main.jpg" alt="TOCABI Humanoid Robot" style="width:100%; max-width:800px; margin: 20px 0;">

---

## Research Motivation

Bipedal humanoid robots face unique challenges compared to quadrupedal systems:
- High center of gravity leading to instability during leg swing
- Large leg inertia from concentrated mass
- Complex contact dynamics during foot placement
- Difficulty maintaining natural gait while adapting to terrain variations

**Project Goal**: Develop a learning-based locomotion controller that enables parkour-like terrain negotiation (stairs, slopes, irregular surfaces) using  heightmap observations.

---

## Technical Approach

### Phase 1: Mixture-of-Experts Architecture

**Initial Hypothesis**: Train terrain-specific expert policies and use a gating network to select appropriate behaviors.

**Implementation**:
- Trained expert policies on 5 terrain types in IsaacGym: smooth slopes, rough slopes, stairs up, stairs down, discrete obstacles
- Used AMP (Adversarial Motion Priors) to learn natural walking on flat terrain
- Attempted fine-tuning on stair terrain with added heightmap observations

**Outcome & Key Learning**:
- Fine-tuning degraded walking motion quality (low episode length, reward collapse)
- **Critical insight**: Simply increasing task reward weight while reducing style reward does not preserve locomotion quality
- Root cause: Existing reward functions lacked explicit constraints for foot height and gait patterns
- **Lesson learned**: Terrain adaptation requires careful reward engineering, not just observation augmentation

### Phase 2: Reward-Based DWL Framework

**Pivot Strategy**: Following mentor guidance, adopted a reward-driven approach inspired by Denoising World Model Learning.

**Architecture Design**:
- Implemented privileged learning with teacher-student framework
- Encoded heightmap using CNN, provided to both observation and privileged information streams
- Used PPO algorithm in IsaacGym with 4096 parallel environments


**Reward Function Development**:
```
Total Reward = Velocity Tracking + Periodic Reward 
               + Foot Trajectory (Quintic Polynomial) 
               + Regularization Terms
```
- **Velocity tracking**: Target forward/lateral velocity and yaw rate
- **Periodic reward**: Encourage natural gait rhythm
- **Foot reward**: Smooth foot trajectories for terrain clearance
- **Regularization**: Joint velocity, acceleration, torque, contact force limits

**Validation on Flat Terrain**:
- Successfully trained stable walking policy
- Confirmed reward function feasibility through visualization
- Established baseline before scaling to complex terrains

### Phase 3: DWL Implementation Challenges

**Attempted Implementation**:
- Built encoder-decoder structure with denoising objective
- Integrated privileged observations (ground-truth base state, contact forces, terrain geometry)
- Added heightmap encoding to latent representations

**Training Issues Encountered**:
- Episode length remained very low (premature terminations)
- Denoising loss diverged significantly
- Actor and critic losses converged too quickly (potential underfitting)

**Ongoing Analysis**:
Currently conducting systematic debugging to identify architectural causes:
- Examining latent space dimensionality and information bottlenecks
- Investigating heightmap encoding quality and spatial resolution
- Analyzing observation normalization and reward scaling
- Considering curriculum learning approaches for progressive difficulty

---

## Technical Contributions

**Reward Engineering**
- Designed hierarchical reward structure balancing multiple objectives
- Validated reward feasibility through iterative flat-terrain testing
- Identified limitations of naive style-task reward balancing

**Architecture Exploration**
- Gained hands-on experience with multiple RL paradigms: AMP, MoE, DWL
- Implemented privileged learning pipeline with teacher-student distillation
- Designed observation spaces incorporating CNN-encoded heightmaps

**Systematic Debugging**
- Developed analytical approach to diagnose training failures
- Identified failure patterns through episode length, loss trajectories, and policy visualizations
- Proposed architectural modifications based on root cause analysis

---

## Key Learnings & Research Insights

**On Reward Design**
Terrain adaptation cannot be achieved through observation augmentation alone. Explicit reward shaping for desired behaviors (foot clearance, gait stability) is essential—a fundamental insight often overlooked in favor of more complex architectures.

**On Failure Analysis**
When DWL training failed, systematic debugging revealed potential issues in latent dimensionality and encoding quality. This experience reinforced that understanding *why* methods fail is as valuable as implementing successful approaches.

**On Research Process**
Working with cutting-edge methods (DWL paper published Aug 2024) during my internship taught me to rapidly adapt research directions based on experimental results and mentor feedback—a critical skill for modern ML research.

---

## Technologies & Tools

**Simulation**: IsaacGym (NVIDIA)  
**Machine Learning**: PyTorch, PPO (Proximal Policy Optimization)  
**Robot Platform**: TOCABI (32 DOF humanoid, DYROS Lab)  
**Observation Processing**: CNN-based heightmap encoding  
**Development**: Python, Linux

---

## Next Steps & Future Work

If DWL training stabilizes through architectural improvements:
1. **RGBD Integration**: Replace simulated heightmap with real sensor data
2. **Sim-to-Real Transfer**: Domain randomization and system identification
3. **Hardware Deployment**: Test learned policy on physical TOCABI robot
4. **Terrain Generalization**: Validate zero-shot transfer to unseen environments

---

## Reflections

This internship taught me that research progress is nonlinear. While I didn't achieve a fully working terrain-adaptive controller, I gained deep understanding of:
- Why seemingly simple ideas (MoE fine-tuning) fail in practice
- The critical role of reward design in shaping learned behaviors  
- How to systematically debug complex RL systems
- The importance of validating components (rewards) before scaling complexity

These lessons in facing and analyzing failure will be invaluable for my future research career.

---

*This project was conducted under the supervision of Professor Jaeheung Park at DYROS Lab, focusing on cutting-edge research in humanoid robotics and reinforcement learning.*