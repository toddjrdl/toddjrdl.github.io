---
title: "Humanoid Locomotion with RL: TOCABI Bipedal Robot"
excerpt: "Reinforcement learning pipeline for stable bipedal walking with privileged observations and sim2real transfer<br/><img src='/images/tocabi-thumbnail.jpg'>"
collection: portfolio
---

## Overview

Development of a robust reinforcement learning pipeline for TOCABI, a full-scale bipedal humanoid robot at DYROS Lab, Seoul National University. This project focused on enabling stable locomotion across diverse terrains through advanced RL techniques and careful sim2real transfer.

<img src="/images/tocabi-main.jpg" alt="TOCABI Humanoid Robot" style="width:100%; max-width:800px; margin: 20px 0;">

## Project Background

Bipedal humanoid robots face fundamental challenges in achieving stable, human-like locomotion due to:
- **High-dimensional state spaces**: 30+ DOF requiring coordinated control
- **Contact dynamics complexity**: Unpredictable ground interactions and impact forces
- **Sim2real gap**: Physical robots behave differently from simulation
- **Sample efficiency**: Real-world training is expensive and potentially damaging

## Technical Approach

### Reinforcement Learning Architecture

**Privileged Learning Framework**
- Teacher policy trained with ground-truth state information (privileged observations)
- Student policy learns from realistic sensor data only
- Knowledge distillation from teacher to student for robust deployment

**Reward Shaping Strategy**
- Base stability rewards: Torso orientation, CoM tracking, foot clearance
- Locomotion efficiency: Forward velocity tracking, energy consumption
- Contact rewards: Smooth foot landings, proper stance timing
- Regularization: Joint position/velocity limits, action smoothness

<img src="/images/tocabi-rl-pipeline.png" alt="RL Training Pipeline" style="width:100%; max-width:700px; margin: 20px 0;">

### State Space Design

**Privileged Observations (Teacher)**
- Ground truth base orientation and velocity
- Precise contact forces and terrain geometry
- Full joint state without sensor noise

**Sensor Observations (Student)**  
- IMU readings (angular velocity, linear acceleration)
- Joint encoders (position, velocity with realistic noise)
- Foot force sensors (with measurement delays)

### Training Infrastructure

**Simulation Environment**
- MuJoCo physics engine for accurate contact dynamics
- Domain randomization across:
  - Joint friction and damping coefficients
  - Ground friction properties
  - Actuator gains and delays
  - Sensor noise and bias
- Parallel simulation with 4096 environments

**Policy Architecture**
- Actor-Critic framework with shared feature extraction
- Neural network: 3 hidden layers (256-128-64 neurons)
- Activation: ELU for smooth gradients
- Training: PPO algorithm with adaptive KL penalty

## Implementation Details

### Debugging & Stability

Key challenges encountered and resolved:
1. **Training instability**: Implemented gradient clipping and learning rate scheduling
2. **Reward scaling**: Balanced competing objectives through careful weight tuning
3. **Catastrophic forgetting**: Added experience replay buffer for rare events
4. **Contact discontinuities**: Smooth contact model with penetration tolerance

### Control Frequency & Latency

- Simulation control loop: 500 Hz
- Policy inference: 100 Hz (decimation factor 5)
- Added realistic 10ms actuation delay for sim2real transfer

## Results

**Simulation Performance**
- ✅ Stable walking at 0.5 m/s on flat terrain
- ✅ Robust to ±10° terrain slopes
- ✅ Recovers from external pushes up to 50N

**Sim2Real Transfer Progress**
- Successfully deployed basic standing controller to hardware
- Walking policy under testing with safety harness
- Identified key domain gaps requiring further randomization

<img src="/images/tocabi-results.gif" alt="Walking Results" style="width:100%; max-width:700px; margin: 20px 0;">

## Technical Contributions

**Reward Engineering**
- Developed hierarchical reward structure balancing stability and efficiency
- Implemented curriculum learning for progressive difficulty increase

**Observation Design**
- Systematic privileged learning framework for sim2real transfer
- Noise modeling based on actual TOCABI sensor specifications

**Training Stability**
- Debugging pipeline for identifying policy failure modes
- Hyperparameter sweep methodology for robust convergence

## Technologies Used

**Simulation**: MuJoCo, Isaac Gym  
**Machine Learning**: PyTorch, Stable-Baselines3, PPO  
**Robot Platform**: TOCABI (32 DOF humanoid robot)  
**Development**: Python, ROS2, C++  
**Analysis**: Weights & Biases, TensorBoard

## Lab & Collaboration

**Research Institution**: [DYROS (Dynamic Robotics Systems Lab)](http://dyros.snu.ac.kr/), Seoul National University  
**Duration**: July 2025 - September 2025  
**My Role**: Research Intern - Reward shaping, privileged observation design, training stability debugging

---

*This project was conducted under the supervision of Professor Jaeheung Park at DYROS Lab, focusing on cutting-edge research in humanoid robotics and reinforcement learning.*